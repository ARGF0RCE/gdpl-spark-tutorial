{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apache Spark with PySpark","text":"<p>Welcome to this code-first tutorial on Apache Spark for large-scale data processing. This site covers:</p> <ul> <li>Theoretical motivations behind Spark and big data processing.</li> <li>Practical, step-by-step instructions on setting up and using Spark.</li> <li>Hands-on examples using PySpark and popular Python libraries like pandas, numpy, scikit-learn, matplotlib, plotly, and statsmodels.</li> </ul> <p></p> <p>Start with the Introduction to learn about the historical context and why Spark is awesome!</p>"},{"location":"introduction/","title":"Introduction &amp; Syllabus","text":""},{"location":"introduction/#why-spark","title":"Why Spark?","text":"<ul> <li>Scalability: Spark can handle huge datasets by distributing work across a cluster.</li> <li>Speed: In-memory computations can be 10-100x faster compared to disk-based solutions like Hadoop MapReduce.</li> <li>Flexibility: PySpark, R, Scala, Java... Spark supports multiple languages.</li> </ul>"},{"location":"introduction/#syllabus-at-a-glance","title":"Syllabus at a Glance","text":"<ol> <li>Introduction and Motivation <ul> <li>Show how Spark beats a naive Python script.</li> </ul> </li> <li>Setting Up Apache Spark <ul> <li>Install Spark locally, in Colab, or on the cloud.</li> </ul> </li> <li>Spark Basics with PySpark <ul> <li>RDDs, DataFrames, transformations, and actions.</li> </ul> </li> <li>Interfacing with Python Libraries <ul> <li>Pandas, Numpy, Scikit-Learn, etc.</li> </ul> </li> </ol> <p>Proceed to the notebooks for hands-on code demos.</p>"},{"location":"notebooks/00-introduction-and-motivation/","title":"0. Introduction and Motivation","text":"In\u00a0[\u00a0]: Copied! <pre>import random\nimport time\n\nN = 10_000_000\ndata = [random.random() for _ in range(N)]\n\nstart = time.time()\nmean_val = sum(data) / len(data)\nend = time.time()\n\nprint(f\"Mean: {mean_val}\")\nprint(f\"Time taken (pure Python): {end - start:.2f} seconds\")\n</pre> import random import time  N = 10_000_000 data = [random.random() for _ in range(N)]  start = time.time() mean_val = sum(data) / len(data) end = time.time()  print(f\"Mean: {mean_val}\") print(f\"Time taken (pure Python): {end - start:.2f} seconds\") In\u00a0[\u00a0]: Copied! <pre>import pyspark\nfrom pyspark.sql import SparkSession\nimport time\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"SparkIntro\") \\\n    .getOrCreate()\n\n# Convert the Python list to an RDD\nrdd = spark.sparkContext.parallelize(data)\n\nstart_spark = time.time()\nmean_val_spark = rdd.mean()\nend_spark = time.time()\n\nprint(f\"Spark Mean: {mean_val_spark}\")\nprint(f\"Time taken (Spark): {end_spark - start_spark:.2f} seconds\")\n\n# Stop the Spark session\nspark.stop()\n</pre> import pyspark from pyspark.sql import SparkSession import time  # Create Spark session spark = SparkSession.builder \\     .appName(\"SparkIntro\") \\     .getOrCreate()  # Convert the Python list to an RDD rdd = spark.sparkContext.parallelize(data)  start_spark = time.time() mean_val_spark = rdd.mean() end_spark = time.time()  print(f\"Spark Mean: {mean_val_spark}\") print(f\"Time taken (Spark): {end_spark - start_spark:.2f} seconds\")  # Stop the Spark session spark.stop()"},{"location":"notebooks/00-introduction-and-motivation/#0-introduction-and-motivation","title":"0. Introduction and Motivation\u00b6","text":"<p>Apache Spark has become one of the most popular frameworks for large-scale data processing and advanced analytics. It is widely adopted by companies such as Netflix, Uber, Airbnb, and more, primarily because:</p> <ul> <li>It offers in-memory computation, which can be significantly faster than traditional disk-based systems.</li> <li>It has a unified engine for both batch and streaming data.</li> <li>It provides high-level APIs in Python, Scala, Java, and R for accessible big data analytics.</li> </ul>"},{"location":"notebooks/00-introduction-and-motivation/#real-world-spark-use-cases","title":"Real-World Spark Use Cases\u00b6","text":"<ol> <li><p>Netflix:</p> <ul> <li>Spark is used to build and run Netflix\u2019s recommendation engine, analyzing user watching patterns and behaviors in near real-time.</li> <li>Streams of event data (when you pause, play, skip, or interact with a video) are ingested, then processed with Spark to generate content suggestions.</li> <li>Spark\u2019s in-memory engine drastically reduces the time to train and refresh large-scale machine learning models.</li> </ul> </li> <li><p>Uber:</p> <ul> <li>Uber leverages Spark (alongside Kafka and other data pipelines) to handle real-time analytics for ride pricing, supply-demand forecasting, and route optimization.</li> <li>Spark\u2019s streaming capabilities help them react quickly to changing traffic conditions, user surge, and more.</li> </ul> </li> <li><p>Airbnb:</p> <ul> <li>Airbnb uses Spark to analyze user bookings, prices, and host preferences.</li> <li>They apply machine learning models on huge datasets for personalized search rankings and dynamic pricing.</li> </ul> </li> <li><p>E-commerce Platforms:</p> <ul> <li>Large online retailers use Spark for recommendation engines, A/B testing analysis, and fraud detection.</li> <li>Data streams (clicks, orders, reviews) can be combined and processed in near real-time for better user experiences.</li> </ul> </li> </ol> <p>Spark\u2019s ability to distribute processing across a cluster allows you to tackle massive datasets that would be infeasible to handle on a single machine. Additionally, Spark\u2019s ecosystem includes libraries like Spark SQL, Spark Streaming, MLlib, and GraphX, which turn Spark into a one-stop solution for broad data processing tasks.</p>"},{"location":"notebooks/00-introduction-and-motivation/#comparing-apache-spark-with-hadoop","title":"Comparing Apache Spark with Hadoop\u00b6","text":"<p>While both Apache Spark and Hadoop are powerful tools for big data processing, they have distinct differences in architecture, performance, and use cases.</p>"},{"location":"notebooks/00-introduction-and-motivation/#diagram-apache-spark-vs-hadoop-architecture","title":"Diagram: Apache Spark vs. Hadoop Architecture\u00b6","text":"<p>Reference for more information: GeeksforGeeks</p>"},{"location":"notebooks/00-introduction-and-motivation/#interpretation","title":"Interpretation\u00b6","text":"<ul> <li><p>Processing Model:</p> <ul> <li>Hadoop: Utilizes a disk-based storage system (HDFS) and processes data in batches using the MapReduce paradigm. Each operation reads from and writes to disk, which can introduce latency.</li> <li>Spark: Employs in-memory processing, keeping data in RAM between operations. This approach significantly reduces read/write cycles to disk, enhancing performance, especially for iterative tasks.</li> </ul> </li> <li><p>Performance:</p> <ul> <li>Hadoop: Suitable for large-scale batch processing but may experience slower performance due to disk I/O operations.</li> <li>Spark: Can be up to 100 times faster than Hadoop for certain tasks, thanks to in-memory computation. This speed advantage is particularly noticeable in machine learning algorithms and real-time data processing.</li> </ul> </li> <li><p>Fault Tolerance:</p> <ul> <li>Hadoop: Achieves fault tolerance through data replication across multiple nodes. If a node fails, data can be retrieved from another replica.</li> <li>Spark: Uses Resilient Distributed Datasets (RDDs) with lineage information, allowing it to recompute lost data without the need for replication, thus saving storage space.</li> </ul> </li> <li><p>Ease of Use:</p> <ul> <li>Hadoop: Requires complex code, often in Java, making it less accessible for rapid development.</li> <li>Spark: Provides high-level APIs in multiple languages, including Python, Scala, and R, simplifying the development process.</li> </ul> </li> </ul>"},{"location":"notebooks/00-introduction-and-motivation/#diagram-performance-comparison","title":"Diagram: Performance Comparison\u00b6","text":"<p>Reference for more information: Data Engineer Academy</p>"},{"location":"notebooks/00-introduction-and-motivation/#interpretation","title":"Interpretation\u00b6","text":"<ul> <li><p>Batch Processing:</p> <ul> <li>Hadoop: Efficient for processing large volumes of data in batches but may not be ideal for real-time analytics.</li> <li>Spark: Excels in both batch and real-time processing, offering flexibility for various data processing needs.</li> </ul> </li> <li><p>Scalability:</p> <ul> <li>Both frameworks are highly scalable, capable of handling petabytes of data across numerous nodes. However, Spark's in-memory requirements can lead to higher costs due to the need for more RAM.</li> </ul> </li> </ul>"},{"location":"notebooks/00-introduction-and-motivation/#use-cases","title":"Use Cases\u00b6","text":"<p>Hadoop use cases include:</p> <ul> <li><p>Processing large datasets in environments where data size exceeds available memory.</p> </li> <li><p>Building data analysis infrastructure with a limited budget.</p> </li> <li><p>Completing jobs where immediate results are not required, and time is not a limiting factor.</p> </li> <li><p>Batch processing with tasks exploiting disk read and write operations.</p> </li> <li><p>Historical and archive data analysis.</p> </li> <li><p>With Spark, we can separate the following use cases where it outperforms Hadoop:</p> </li> <li><p>The analysis of real-time stream data.</p> </li> <li><p>When time is of the essence, Spark delivers quick results with in-memory computations.</p> </li> <li><p>Dealing with the chains of parallel operations using iterative algorithms.</p> </li> <li><p>Graph-parallel processing to model the data.</p> </li> <li><p>All machine learning applications.</p> </li> </ul> <p>Reference for more information: PhoenixNAP</p>"},{"location":"notebooks/00-introduction-and-motivation/#interpretation","title":"Interpretation\u00b6","text":"<ul> <li>Machine Learning:<ul> <li>Hadoop: Limited machine learning capabilities through libraries like Mahout. Not ideal for iterative algorithms.</li> <li>Spark: Offers MLlib for scalable machine learning tasks, supporting iterative algorithms and real-time processing.</li> </ul> </li> </ul>"},{"location":"notebooks/00-introduction-and-motivation/#real-world-spark-use-cases","title":"Real-World Spark Use Cases\u00b6","text":"<ol> <li>Netflix: Recommendation systems, real-time user behavior analysis.</li> <li>Uber: Real-time analytics for pricing, supply-demand forecasting.</li> <li>Airbnb: User personalization, dynamic pricing.</li> <li>E-commerce Platforms: Fraud detection, recommendation engines.</li> </ol>"},{"location":"notebooks/00-introduction-and-motivation/#simple-python-example","title":"Simple Python Example\u00b6","text":"<p>Below is a trivial code snippet that processes a random dataset with pure Python loops or standard libraries.</p>"},{"location":"notebooks/00-introduction-and-motivation/#simple-spark-example","title":"Simple Spark Example\u00b6","text":"<p>Now let's see how Spark can help us handle even larger data seamlessly, leveraging distributed computing.</p>"},{"location":"notebooks/01-setting-up-apache-spark/","title":"1. Setting Up Apache Spark","text":"LinuxmacOSWindows NativeWindows WSL2"},{"location":"notebooks/01-setting-up-apache-spark/#linux-installation-ubuntudebiancentosfedora","title":"Linux Installation (Ubuntu/Debian/CentOS/Fedora)","text":"<ol> <li>Check Java Installation:</li> </ol> <pre><code>java -version\n</code></pre> <p>If Java isn\u2019t installed or you need a different version, install OpenJDK:</p> <pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install openjdk-8-jdk -y\n\n# Fedora/CentOS\nsudo yum install java-1.8.0-openjdk -y\n</code></pre> <ol> <li> <p>Download Spark Prebuilt Binaries:</p> </li> <li> <p>Go to https://spark.apache.org/downloads.html</p> </li> <li>Choose a Spark release (e.g., Spark 3.3.x) and a package type built for Hadoop (e.g., Hadoop 3.3).</li> <li> <p>Download the <code>.tgz</code> file.</p> </li> <li> <p>Extract Spark:</p> </li> </ol> <pre><code>tar xvf spark-3.3.0-bin-hadoop3.tgz\nmv spark-3.3.0-bin-hadoop3/ ~/spark\n</code></pre> <ol> <li>Add Spark to PATH: Append the following to your <code>~/.bashrc</code> (or <code>~/.zshrc</code>, depending on your shell):</li> </ol> <pre><code>export SPARK_HOME=\"$HOME/spark\"\nexport PATH=\"$SPARK_HOME/bin:$PATH\"\n</code></pre> <p>Then reload:</p> <pre><code>source ~/.bashrc\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <pre><code>spark-shell --version\n# or\npyspark --version\n</code></pre> <p>You should see Spark version info printed to the console.</p>"},{"location":"notebooks/01-setting-up-apache-spark/#macos-installation","title":"macOS Installation","text":"<ol> <li>Install Homebrew (Optional but Recommended): Homebrew simplifies package installation on macOS. If you don\u2019t have it:</li> </ol> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <ol> <li>Install Java:</li> </ol> <pre><code>brew update\nbrew install openjdk@8\n# or brew install openjdk@11\n</code></pre> <p>Then add Java to your PATH (Homebrew will often print instructions to do so).</p> <ol> <li> <p>Download Spark Prebuilt Binaries:</p> </li> <li> <p>Visit https://spark.apache.org/downloads.html</p> </li> <li>Select the latest Spark version (e.g., 3.3.x) and the Hadoop build.</li> <li> <p>Download the <code>.tgz</code> archive.</p> </li> <li> <p>Extract and Move Spark:</p> </li> </ol> <pre><code>tar xvf spark-3.3.0-bin-hadoop3.tgz\nmv spark-3.3.0-bin-hadoop3/ ~/spark\n</code></pre> <ol> <li>Set Environment Variables: Append to your <code>~/.zshrc</code> or <code>~/.bash_profile</code>:</li> </ol> <pre><code>export SPARK_HOME=\"$HOME/spark\"\nexport PATH=\"$SPARK_HOME/bin:$PATH\"\n</code></pre> <p>Then reload:</p> <pre><code>source ~/.zshrc  # or source ~/.bash_profile\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <pre><code>pyspark --version\n</code></pre> <p>Spark version info should appear.</p>"},{"location":"notebooks/01-setting-up-apache-spark/#windows-native-installation","title":"Windows (Native) Installation","text":"<ol> <li> <p>Install Java:</p> </li> <li> <p>Download Adoptium Temurin or Oracle JDK for Windows (prefer Java 8 or 11).</p> </li> <li>Run the installer. By default, it installs in <code>C:\\Program Files\\Java\\...</code></li> <li>Ensure <code>JAVA_HOME</code> is set. Go to Control Panel \u2192 System \u2192 Advanced system settings \u2192 Environment Variables, add:</li> <li>Variable name: <code>JAVA_HOME</code></li> <li>Variable value: <code>C:\\Program Files\\Java\\jdk1.8.0_xxx</code> (for example)</li> <li> <p>Add <code>%JAVA_HOME%\\bin</code> to your Path.</p> </li> <li> <p>Download Spark Prebuilt Binaries:</p> </li> <li> <p>From https://spark.apache.org/downloads.html, pick a package built for Hadoop 3.x.</p> </li> <li> <p>Unzip the downloaded <code>.tgz</code> file using 7-Zip or WinRAR.</p> </li> <li> <p>Place Spark Folder:</p> </li> <li> <p>Move the unzipped folder to a convenient location, e.g. <code>C:\\spark</code>.</p> </li> <li> <p>Set Environment Variables:</p> </li> <li> <p>Under System Properties \u2192 Environment Variables, create a new user or system variable:</p> </li> <li>Variable name: <code>SPARK_HOME</code></li> <li>Variable value: <code>C:\\spark</code></li> <li> <p>Add <code>C:\\spark\\bin</code> to your Path.</p> </li> <li> <p>Install Python (if not installed):</p> </li> <li> <p>Visit python.org/downloads and install Python 3.7+.</p> </li> <li> <p>Ensure you check \"Add Python to PATH\" during installation.</p> </li> <li> <p>Verify Installation:</p> </li> <li> <p>Open Command Prompt or PowerShell and run: <pre><code>pyspark --version\n</code></pre></p> </li> <li>Spark version info should appear.</li> </ol>"},{"location":"notebooks/01-setting-up-apache-spark/#windows-wsl2-ubuntu-installation","title":"Windows WSL2 (Ubuntu) Installation","text":"<ol> <li> <p>Confirm WSL2 Setup:</p> </li> <li> <p>Install Windows Subsystem for Linux and choose Ubuntu 20.04 or later.</p> </li> <li> <p>Launch the Ubuntu terminal.</p> </li> <li> <p>Check/Install Java:</p> </li> </ol> <pre><code>java -version\n# If needed:\nsudo apt-get update\nsudo apt-get install openjdk-8-jdk\n</code></pre> <ol> <li>Download Spark Prebuilt Binaries:</li> </ol> <pre><code># Inside Ubuntu WSL2\nwget https://downloads.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n</code></pre> <ol> <li>Extract Spark:</li> </ol> <pre><code>tar xvf spark-3.3.0-bin-hadoop3.tgz\nmv spark-3.3.0-bin-hadoop3/ ~/spark\n</code></pre> <ol> <li>Set Environment Variables:</li> </ol> <pre><code>echo \"export SPARK_HOME=\\\"$HOME/spark\\\"\" &gt;&gt; ~/.bashrc\necho \"export PATH=\\\"$SPARK_HOME/bin:\\$PATH\\\"\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <pre><code>pyspark --version\n</code></pre> <p>Spark version info should appear.</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/","title":"2. Spark Basics with PySpark","text":"In\u00a0[\u00a0]: Copied! <pre>from pyspark.sql import SparkSession\n\n# Create or get a Spark session\nspark = SparkSession.builder \\\n    .appName(\"SparkBasics\") \\\n    .getOrCreate()\n\nprint(\"SparkSession created!\")\n</pre> from pyspark.sql import SparkSession  # Create or get a Spark session spark = SparkSession.builder \\     .appName(\"SparkBasics\") \\     .getOrCreate()  print(\"SparkSession created!\") <p>Note: If you\u2019re running this locally, ensure <code>SPARK_HOME</code> is set correctly and <code>pyspark</code> is installed. On a cloud environment or in a notebook like Databricks, a SparkSession may already be created for you.</p> In\u00a0[\u00a0]: Copied! <pre># Create an RDD by parallelizing a local list\ndata_list = [\"apple\", \"banana\", \"cherry\", \"date\"]\nrdd = spark.sparkContext.parallelize(data_list)\nprint(\"RDD count:\", rdd.count())\nprint(\"RDD sample:\", rdd.take(2))  # take(2) fetches first 2 elements\n</pre> # Create an RDD by parallelizing a local list data_list = [\"apple\", \"banana\", \"cherry\", \"date\"] rdd = spark.sparkContext.parallelize(data_list) print(\"RDD count:\", rdd.count()) print(\"RDD sample:\", rdd.take(2))  # take(2) fetches first 2 elements <p>Additional Explanation</p> <p>RDDs are the fundamental data structure at the heart of Spark\u2019s distributed engine. Each RDD is conceptually an immutable collection of data partitioned across nodes in the cluster. Spark automatically tracks the lineage of each RDD\u2014that is, the sequence of operations used to create it\u2014making it easy to recompute partitions if some node fails.</p> <p>You can also create RDDs from:</p> <ul> <li>Text files:<pre>text_rdd = spark.sparkContext.textFile(\"path/to/myfile.txt\")\n</pre> </li> <li>Whole directories of data:<pre>large_rdd = spark.sparkContext.textFile(\"hdfs://path/to/huge-dataset/*\")\n</pre> </li> </ul> <p>When working locally, you can load files from your machine\u2019s filesystem. For cluster deployments, you typically load from HDFS, S3, or another distributed store.</p> In\u00a0[\u00a0]: Copied! <pre># RDD Transformations\nmapped_rdd = rdd.map(lambda x: x.upper())   # map() transforms each element\nfiltered_rdd = mapped_rdd.filter(lambda x: x.startswith(\"B\"))  # filter() keeps certain elements\n\n# RDD Action\nresult = filtered_rdd.collect()  # collect() returns all elements to the driver\n\nprint(\"Transformed RDD Result:\", result)\n</pre> # RDD Transformations mapped_rdd = rdd.map(lambda x: x.upper())   # map() transforms each element filtered_rdd = mapped_rdd.filter(lambda x: x.startswith(\"B\"))  # filter() keeps certain elements  # RDD Action result = filtered_rdd.collect()  # collect() returns all elements to the driver  print(\"Transformed RDD Result:\", result) <p>Additional Explanation</p> <ul> <li>Transformations: Create a new RDD by defining how each record is mapped from the parent RDD. Examples include:<ul> <li><code>map()</code>: Apply a function to each element.</li> <li><code>filter()</code>: Keep only elements passing a predicate.</li> <li><code>flatMap()</code>: Similar to <code>map()</code> but allows splitting elements into multiple outputs.</li> </ul> </li> <li>Actions: Trigger execution and return a value. Examples include:<ul> <li><code>reduce(func)</code>: Combine elements using a user-specified function that operates on two items at a time.</li> <li><code>count()</code>: Return the number of elements in the RDD.</li> <li><code>first()</code>, <code>take(n)</code>: Retrieve elements to the driver program.</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>sample_data = [\n    (\"Alice\", 29, \"Engineer\"),\n    (\"Bob\",   35, \"Doctor\"),\n    (\"Cathy\", 25, \"Artist\")\n]\n\ncolumns = [\"Name\", \"Age\", \"Occupation\"]\ndf = spark.createDataFrame(sample_data, columns)\n\ndf.show()\n</pre> sample_data = [     (\"Alice\", 29, \"Engineer\"),     (\"Bob\",   35, \"Doctor\"),     (\"Cathy\", 25, \"Artist\") ]  columns = [\"Name\", \"Age\", \"Occupation\"] df = spark.createDataFrame(sample_data, columns)  df.show() <p>Additional Explanation</p> <p>While you can create DataFrames from local data, real-world use often comes from:</p> <ol> <li>Reading CSV/JSON/Parquet:<pre>df_csv = spark.read.csv(\"path/to/data.csv\", header=True, inferSchema=True)\ndf_parquet = spark.read.parquet(\"path/to/data.parquet\")\n</pre> </li> <li>SQL Tables (via JDBC/ODBC or Hive Metastore):<pre>jdbc_df = spark.read \\\n .format(\"jdbc\") \\\n .option(\"url\", \"jdbc:postgresql://hostname/db\") \\\n .option(\"dbtable\", \"tablename\") \\\n .option(\"user\", \"username\") \\\n .option(\"password\", \"secret\") \\\n .load()\n</pre> </li> </ol> <p>DataFrames also enable quick ETL (Extract, Transform, Load) patterns if you need to combine multiple datasets. By specifying the schema or letting Spark infer it, you keep metadata about column names, types, etc.</p> <p>Under the hood, Spark still uses RDDs for low-level operations. But for day-to-day usage, DataFrames are often more concise and more performant thanks to the Catalyst Optimizer.</p> In\u00a0[\u00a0]: Copied! <pre># Print schema\ndf.printSchema()\n\n# Summaries\ndf.describe().show()\n</pre> # Print schema df.printSchema()  # Summaries df.describe().show() <p>Additional Explanation</p> <p>You can also use:</p> <ul> <li><code>df.columns</code> to get a list of column names.</li> <li><code>df.dtypes</code> to see columns and their Spark data types.</li> <li><code>df.head(n)</code> or <code>df.take(n)</code> to return the first <code>n</code> rows locally (similar to <code>.show(n)</code> but returns a list of Row objects).</li> </ul> <p>Example:</p> <pre>print(\"Columns:\", df.columns)\nprint(\"Schema (dtypes):\", df.dtypes)\nfor row in df.head(2):\n    print(row)\n</pre> <p>These methods make it easy to quickly inspect your DataFrame structure and sample data.</p> In\u00a0[\u00a0]: Copied! <pre># Select columns\ndf.select(\"Name\", \"Age\").show()\n\n# Filter rows\ndf.filter(df.Age &gt; 30).show()\n\n# Group + Agg\ndf.groupBy(\"Occupation\").count().show()\n</pre> # Select columns df.select(\"Name\", \"Age\").show()  # Filter rows df.filter(df.Age &gt; 30).show()  # Group + Agg df.groupBy(\"Occupation\").count().show() <p>Additional Explanation</p> <ul> <li>Column Expressions: You can reference columns via <code>df.colName</code> or using Spark\u2019s <code>F</code> (functions) library:<pre>from pyspark.sql.functions import col, lower, upper\nnew_df = df.select(upper(col(\"Name\")).alias(\"NAME_UPPER\"))\nnew_df.show()\n</pre> </li> <li>User-Defined Functions (UDFs): For more complex transformations, define UDFs:<pre>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\ndef greet(name):\n    return f\"Hello, {name}!\"\n\ngreet_udf = udf(greet, StringType())\ndf.select(\"Name\", greet_udf(col(\"Name\")).alias(\"Greeting\")).show()\n</pre> </li> </ul> <p>Note: However, keep in mind that UDFs can be slower than built-in Spark functions, since they bypass many of Spark\u2019s optimizations.</p> <ul> <li><p>Joins: DataFrames support <code>inner</code>, <code>left</code>, <code>right</code>, <code>full</code>, and <code>cross</code> joins. Example:</p> <pre>df1.join(df2, df1.id == df2.id, \"inner\").show()\n</pre> <p>Use the join mode that suits your data relationship to handle missing or unmatched records appropriately.</p> </li> </ul> <p>Additional Explanation</p> <ul> <li>Transformations: Spark builds a logical plan describing how to transform data. For instance:<ul> <li><code>select()</code>, <code>filter()</code>, <code>withColumn()</code>, <code>groupBy()</code>, <code>agg()</code></li> </ul> </li> <li>Actions: Evaluate the plan. Examples:<ul> <li><code>collect()</code>, <code>count()</code>, <code>show()</code>, <code>head()</code></li> </ul> </li> </ul> <p>Catalyst Query Optimizer Spark\u2019s DataFrame operations are compiled down to an optimized plan thanks to the Catalyst optimizer. It can rearrange filters, push down predicates, and combine operations for efficiency. This is why using DataFrame APIs is typically faster than manual RDD manipulations for most SQL-like workflows.</p> <p>Example:</p> In\u00a0[\u00a0]: Copied! <pre>df_filtered = df.filter(col(\"Age\") &gt; 30)\nrow_count = df_filtered.count()  # triggers execution\nprint(f\"Number of records with Age &gt; 30: {row_count}\")\n</pre> df_filtered = df.filter(col(\"Age\") &gt; 30) row_count = df_filtered.count()  # triggers execution print(f\"Number of records with Age &gt; 30: {row_count}\") In\u00a0[\u00a0]: Copied! <pre># Example of caching a DataFrame\ndf.cache()  # or df.persist()\n# Now subsequent actions on df will be faster if the data is reused\n</pre> # Example of caching a DataFrame df.cache()  # or df.persist() # Now subsequent actions on df will be faster if the data is reused <p>Additional Explanation</p> <ul> <li>Partitioning: Spark splits data into partitions which are processed in parallel across the cluster (or local CPU cores). If you have a large dataset, you can adjust the number of partitions:<pre>df = df.repartition(8)  # Increase partition count\n</pre> </li> <li>Conversely, you can reduce partitions using coalesce() if you want fewer, bigger partitions. Partitioning affects shuffle performance and parallelism.</li> </ul> <p>Persistence Levels: By default, <code>cache()</code> uses MEMORY_ONLY storage. You can choose other persistence levels:</p> <ul> <li><code>MEMORY_AND_DISK</code></li> <li><code>MEMORY_ONLY_SER</code></li> <li><code>DISK_ONLY</code></li> </ul> <p>Depending on data size and memory constraints, picking the right level is essential for performance. For instance, if your data is larger than available memory, <code>MEMORY_AND_DISK</code> can help avoid <code>OutOfMemoryError</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from pyspark import StorageLevel\ndf.persist(StorageLevel.MEMORY_AND_DISK)  # or MEMORY_ONLY, MEMORY_ONLY_SER, etc.\n</pre> from pyspark import StorageLevel df.persist(StorageLevel.MEMORY_AND_DISK)  # or MEMORY_ONLY, MEMORY_ONLY_SER, etc. In\u00a0[\u00a0]: Copied! <pre>spark.stop()\nprint(\"Spark session stopped.\")\n</pre> spark.stop() print(\"Spark session stopped.\") <p>Additional Explanation</p> <p>When you call <code>spark.stop()</code>, Spark attempts to gracefully terminate all active jobs and release resources like executors and shuffle files. In production or cluster environments, you might schedule your Spark job to run, produce output, then stop once the job completes.</p> <p>If you\u2019re in a multi-notebook environment (like Databricks), be mindful that calling <code>stop()</code> can affect other notebooks sharing the same SparkSession. Some cluster managers automatically handle session lifecycles, so consult the platform\u2019s documentation if you\u2019re unsure.</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#2-spark-basics-with-pyspark","title":"2. Spark Basics with PySpark\u00b6","text":"<p>In this notebook, we\u2019ll learn the building blocks of Spark using PySpark:</p> <ol> <li>SparkSession: The entry point to Spark.</li> <li>RDD (Resilient Distributed Datasets): The lower-level abstraction.</li> <li>DataFrames: The higher-level, SQL-like abstraction.</li> <li>Transformations and Actions: The functional operations that drive Spark computations.</li> <li>Partitioning and Persistence: How Spark manages data distribution.</li> </ol> <p>By the end, you\u2019ll understand the basics of how PySpark organizes and processes data at scale.</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#1-creating-a-sparksession","title":"1. Creating a SparkSession\u00b6","text":"<p>The SparkSession is Spark\u2019s main entry point in Spark 2.0+. It allows you to create DataFrames, register DataFrame tables, execute SQL queries, and read from external data sources.</p> <p>Additional Explanation</p> <p>Before Spark 2.0, you would initialize Spark with separate entries (e.g., <code>SparkContext</code>, <code>SQLContext</code>, <code>HiveContext</code>). The <code>SparkSession</code> conveniently unified these contexts into a single entry point. This consolidation means you can seamlessly switch between DataFrame operations, SQL queries, and lower-level RDD manipulations without re-initializing or juggling multiple contexts.</p> <p>When you build your SparkSession, you can also configure:</p> <ul> <li>Master URL (e.g., <code>local[*]</code>, <code>spark://...</code>) to specify the cluster manager.</li> <li>Config options like memory usage, shuffle partitions, or log level.</li> </ul> <p>For example, to set the log level lower (for less verbose output) and specify 4 local cores:</p> <pre>spark = SparkSession.builder \\\n    .appName(\"SparkBasics\") \\\n    .master(\"local[4]\") \\\n    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n    .getOrCreate()\n\nspark.sparkContext.setLogLevel(\"ERROR\")\n</pre> <p>Below, we create a SparkSession named <code>SparkBasics</code>:</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#2-understanding-rdds-resilient-distributed-datasets","title":"2. Understanding RDDs (Resilient Distributed Datasets)\u00b6","text":"<p>RDDs are the lower-level abstraction in Spark, representing a distributed collection of items. They provide fault tolerance (resilience) and can be operated on in parallel across a cluster. Nowadays, you\u2019ll mostly use DataFrames, but RDDs are still useful for specialized or custom operations.</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#21-creating-rdds","title":"2.1 Creating RDDs\u00b6","text":"<p>You can create an RDD in several ways:</p> <ol> <li>Parallelize a local collection (e.g., a Python list).</li> <li>Load external data (text files, CSV, etc.) via <code>sparkContext.textFile(...)</code>.</li> </ol> <p>Below, we create an RDD from a local list:</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#22-transformations-and-actions-on-rdds","title":"2.2 Transformations and Actions on RDDs\u00b6","text":"<p>Spark\u2019s transformations return a new RDD (they\u2019re lazy), while actions trigger execution and return a value (materializing the result). Common transformations include <code>map()</code>, <code>filter()</code>, and <code>flatMap()</code>. Common actions are <code>collect()</code>, <code>count()</code>, <code>reduce()</code>, etc.</p> <p>Below, we apply transformations to an RDD and then use an action:</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#3-dataframes-the-higher-level-abstraction","title":"3. DataFrames: The Higher-Level Abstraction\u00b6","text":"<p>DataFrames build on top of RDDs and provide a relational view of data, with named columns and powerful optimizations via the Catalyst query optimizer. They\u2019re generally faster and easier to use for most big data tasks.</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#31-creating-a-dataframe","title":"3.1 Creating a DataFrame\u00b6","text":"<p>You can create a DataFrame from:</p> <ul> <li>Python lists (small data) or RDDs.</li> <li>External data sources (CSV, JSON, Parquet, etc.).</li> </ul> <p>Here\u2019s a simple example from a Python list of tuples:</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#32-inspecting-dataframes","title":"3.2 Inspecting DataFrames\u00b6","text":"<p>Use methods like <code>.show()</code>, <code>.describe()</code>, and <code>.printSchema()</code> to explore DataFrames:</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#33-dataframe-operations","title":"3.3 DataFrame Operations\u00b6","text":"<p>DataFrames support a wide variety of operations, including selecting columns, filtering, grouping, and aggregation. Many of these are similar to SQL queries.</p> <p>Below is an example:</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#4-transformations-and-actions-on-dataframes","title":"4. Transformations and Actions on DataFrames\u00b6","text":"<p>While RDDs have functional transformations, DataFrames offer a more SQL-like syntax for transformations. Actions (like <code>.show()</code>) will execute the query plan.</p> <p>Example: Filtering rows, creating new columns, or performing aggregations are transformations. Calling <code>.collect()</code> or <code>.count()</code> is an action.</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#5-partitioning-and-persistence","title":"5. Partitioning and Persistence\u00b6","text":"<p>Spark automatically partitions DataFrames/RDDs across the cluster. On a single machine or small environment, you might not notice it as much, but in a distributed setting, partitioning is crucial for parallelism.</p> <p>You can cache or persist frequently accessed data to improve performance:</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#6-shutting-down-spark","title":"6. Shutting Down Spark\u00b6","text":"<p>When you\u2019re finished, it\u2019s good practice to stop the SparkSession (especially in scripts or local dev environments):</p>"},{"location":"notebooks/02-spark-basics-with-pyspark/#summary","title":"Summary\u00b6","text":"<ul> <li>RDDs: Low-level data abstraction, good for custom or specialized tasks.</li> <li>DataFrames: Higher-level relational abstraction, recommended for most analytics.</li> <li>Transformations: Lazy operations that define a computation plan.</li> <li>Actions: Trigger execution and return results.</li> <li>Partitioning and Persistence: Key to scalability and performance.</li> </ul> <p>Now that you know the basics of Spark with PySpark, you can start using Spark to handle large datasets, run queries, and transform data quickly. In the next notebook, we\u2019ll explore how to interface Spark with pandas, numpy, scikit-learn, and more!</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/","title":"3. Interfacing with Python Libraries","text":"In\u00a0[\u00a0]: Copied! <pre>from pyspark.sql import SparkSession\nimport pandas as pd\n\nspark = SparkSession.builder.appName(\"InterfacingWithPythonLibs\").getOrCreate()\n\n# Example Spark DataFrame\ndata = [\n    (\"Alice\", 29, \"Engineer\"),\n    (\"Bob\",   35, \"Doctor\"),\n    (\"Cathy\", 25, \"Artist\"),\n    (\"David\", 42, \"Chef\")\n]\ncolumns = [\"Name\", \"Age\", \"Occupation\"]\n\nspark_df = spark.createDataFrame(data, columns)\nprint(\"Spark DataFrame:\")\nspark_df.show()\n\n# Convert to a pandas DataFrame (Collects all data locally!)\npdf = spark_df.toPandas()\nprint(\"\\nConverted to pandas DataFrame:\")\nprint(pdf)\n</pre> from pyspark.sql import SparkSession import pandas as pd  spark = SparkSession.builder.appName(\"InterfacingWithPythonLibs\").getOrCreate()  # Example Spark DataFrame data = [     (\"Alice\", 29, \"Engineer\"),     (\"Bob\",   35, \"Doctor\"),     (\"Cathy\", 25, \"Artist\"),     (\"David\", 42, \"Chef\") ] columns = [\"Name\", \"Age\", \"Occupation\"]  spark_df = spark.createDataFrame(data, columns) print(\"Spark DataFrame:\") spark_df.show()  # Convert to a pandas DataFrame (Collects all data locally!) pdf = spark_df.toPandas() print(\"\\nConverted to pandas DataFrame:\") print(pdf)  <p>The above code snippet:</p> <ul> <li>Creates a simple Spark DataFrame with 4 rows.</li> <li>Uses <code>.toPandas()</code> to bring it to the driver machine.</li> </ul> <p>In real-world scenarios, you might filter or sample your Spark DataFrame first, so that only a manageable subset is converted to pandas.</p> In\u00a0[\u00a0]: Copied! <pre># Let's assume you already have a local pandas DataFrame 'pdf'\npdf_extra = pd.DataFrame({\n    'Name': [\"Evelyn\", \"Frank\"],\n    'Age': [31, 28],\n    'Occupation': [\"Manager\", \"Nurse\"]\n})\n\n# Convert to Spark DataFrame\nspark_df_extra = spark.createDataFrame(pdf_extra)\n\nprint(\"New Spark DataFrame created from pandas:\")\nspark_df_extra.show()\n</pre> # Let's assume you already have a local pandas DataFrame 'pdf' pdf_extra = pd.DataFrame({     'Name': [\"Evelyn\", \"Frank\"],     'Age': [31, 28],     'Occupation': [\"Manager\", \"Nurse\"] })  # Convert to Spark DataFrame spark_df_extra = spark.createDataFrame(pdf_extra)  print(\"New Spark DataFrame created from pandas:\") spark_df_extra.show()  In\u00a0[\u00a0]: Copied! <pre>import pyspark.pandas as ps\n\n# Create a pandas-on-Spark DataFrame\npsdf = ps.DataFrame({\n    'colA': [10, 20, 30],\n    'colB': [100, 200, 300]\n})\nprint(\"pandas-on-Spark DataFrame:\")\nprint(psdf)\n\n# You can do typical pandas operations, e.g.,\nprint(\"\\npsdf.sum():\")\nprint(psdf.sum())\n</pre> import pyspark.pandas as ps  # Create a pandas-on-Spark DataFrame psdf = ps.DataFrame({     'colA': [10, 20, 30],     'colB': [100, 200, 300] }) print(\"pandas-on-Spark DataFrame:\") print(psdf)  # You can do typical pandas operations, e.g., print(\"\\npsdf.sum():\") print(psdf.sum()) <p>Behind the scenes, <code>pyspark.pandas</code> transforms your calls into Spark transformations. This allows scaling out to large datasets that exceed normal pandas memory constraints.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\n# Suppose 'spark_df' is large. We'll create a smaller sample.\nsmall_spark_df = spark_df.limit(2)  # just 2 rows for demonstration\n\n# Collect to driver as a list of Row objects\nrows = small_spark_df.collect()\n\n# Convert rows to a structured numpy array, or just a list for further processing\nnames = [row['Name'] for row in rows]\nages = np.array([row['Age'] for row in rows])\n\nprint(\"Names (list):\", names)\nprint(\"Ages (numpy array):\", ages)\n</pre> import numpy as np  # Suppose 'spark_df' is large. We'll create a smaller sample. small_spark_df = spark_df.limit(2)  # just 2 rows for demonstration  # Collect to driver as a list of Row objects rows = small_spark_df.collect()  # Convert rows to a structured numpy array, or just a list for further processing names = [row['Name'] for row in rows] ages = np.array([row['Age'] for row in rows])  print(\"Names (list):\", names) print(\"Ages (numpy array):\", ages) <p>If your dataset is large, you might instead prefer distributed transformations in Spark MLlib. But for smaller tasks or specialized numeric calculations, this approach is straightforward.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Suppose we have a Spark DF with numeric columns: e.g., 'Feature1', 'Feature2', 'Label'\nsample_data_for_sklearn = [\n    (1.0, 10.0,  8.5),\n    (2.0, 18.0, 14.0),\n    (3.0, 23.0, 17.2),\n    (4.0, 28.0, 20.0),\n    (5.0, 39.0, 32.0)\n]\nschema_cols = [\"Feature1\", \"Feature2\", \"Label\"]\ndf_sklearn = spark.createDataFrame(sample_data_for_sklearn, schema_cols)\n\n# Collect to driver in pandas\npdf_sklearn = df_sklearn.toPandas()\n\n# Separate features and labels\nX = pdf_sklearn[[\"Feature1\", \"Feature2\"]]\ny = pdf_sklearn[\"Label\"]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit scikit-learn model\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# Evaluate\nscore = lr_model.score(X_test, y_test)\nprint(\"R-squared on test set:\", score)\nprint(\"Coefficients:\", lr_model.coef_)\nprint(\"Intercept:\", lr_model.intercept_)\n</pre> from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  # Suppose we have a Spark DF with numeric columns: e.g., 'Feature1', 'Feature2', 'Label' sample_data_for_sklearn = [     (1.0, 10.0,  8.5),     (2.0, 18.0, 14.0),     (3.0, 23.0, 17.2),     (4.0, 28.0, 20.0),     (5.0, 39.0, 32.0) ] schema_cols = [\"Feature1\", \"Feature2\", \"Label\"] df_sklearn = spark.createDataFrame(sample_data_for_sklearn, schema_cols)  # Collect to driver in pandas pdf_sklearn = df_sklearn.toPandas()  # Separate features and labels X = pdf_sklearn[[\"Feature1\", \"Feature2\"]] y = pdf_sklearn[\"Label\"]  # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Fit scikit-learn model lr_model = LinearRegression() lr_model.fit(X_train, y_train)  # Evaluate score = lr_model.score(X_test, y_test) print(\"R-squared on test set:\", score) print(\"Coefficients:\", lr_model.coef_) print(\"Intercept:\", lr_model.intercept_) <p>When you run the above:</p> <ul> <li>Spark will collect all rows (5 in this example) into a pandas DataFrame.</li> <li>We use <code>train_test_split</code> and <code>LinearRegression</code> from scikit-learn to fit a simple model.</li> </ul> <p>Again, this is only advisable for small to moderate datasets that your driver machine can handle in memory.</p> In\u00a0[\u00a0]: Copied! <pre># (Optional) Stop the Spark session when you're done.\nspark.stop()\nprint(\"Spark session stopped.\")\n</pre> # (Optional) Stop the Spark session when you're done. spark.stop() print(\"Spark session stopped.\")"},{"location":"notebooks/03-interfacing-with-python-libraries/#3-interfacing-with-python-libraries","title":"3. Interfacing with Python Libraries\u00b6","text":"<p>In this notebook, we will explore how Apache Spark integrates with common Python data science libraries, specifically:</p> <ol> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> </ol> <p>You\u2019ll learn:</p> <ul> <li>How to convert Spark DataFrames to and from pandas DataFrames.</li> <li>How to leverage PySpark for distributing data across your cluster.</li> <li>When (and why) you\u2019d want to collect smaller datasets locally to do numpy-based manipulation.</li> <li>Comparisons between Spark MLlib and scikit-learn.</li> <li>Techniques for scaling your machine learning tasks beyond a single node.</li> </ul> <p>By the end, you\u2019ll have a robust understanding of how Spark can coexist in the typical Python data science ecosystem.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#0-prerequisites","title":"0. Prerequisites\u00b6","text":"<p>This notebook assumes:</p> <ul> <li>You have a SparkSession available as <code>spark</code>.</li> <li>You have installed relevant Python libraries (<code>pandas</code>, <code>numpy</code>, <code>scikit-learn</code>).</li> <li>You understand basic Spark concepts like DataFrames, RDDs, transformations, and actions.</li> </ul> <p>If you haven\u2019t already, please refer to previous notebooks for setup and Spark basics.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#1-pandas-and-spark","title":"1. pandas and Spark\u00b6","text":""},{"location":"notebooks/03-interfacing-with-python-libraries/#11-converting-spark-dataframes-to-pandas-dataframes","title":"1.1 Converting Spark DataFrames to pandas DataFrames\u00b6","text":"<p>Often, you\u2019ll want to perform local operations on a smaller subset of data that easily fits in memory on a single machine. In such scenarios, you can collect that data as a pandas DataFrame. The <code>toPandas()</code> method is the simplest approach for doing so.</p> <p>Warning: <code>toPandas()</code> will bring all data into the driver\u2019s memory, so ensure the dataset isn\u2019t too large.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#12-converting-pandas-dataframes-to-spark-dataframes","title":"1.2 Converting pandas DataFrames to Spark DataFrames\u00b6","text":"<p>If you have local data in pandas (for example, from CSV files that are small enough to fit on a single machine), you can create a Spark DataFrame using the <code>spark.createDataFrame()</code> method.</p> <p>Tip: Some overhead exists when converting from pandas to Spark\u2014Spark has to infer or confirm the schema. For large data, consider loading directly into Spark from the source instead of going through pandas.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#13-pandas-api-on-spark-pysparkpandas","title":"1.3 pandas API on Spark (pyspark.pandas)\u00b6","text":"<p>Starting in Spark 3.2+, you can use pandas APIs on Spark (often referred to as <code>pyspark.pandas</code>). This is an experimental feature that attempts to let you write pandas-like code but execute it distributively on Spark. That way, you can handle bigger-than-memory data with familiar pandas syntax.</p> <p>Note: Performance might differ from pure Spark SQL, but for many users who love pandas, this can ease the learning curve.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#2-numpy-and-spark","title":"2. numpy and Spark\u00b6","text":""},{"location":"notebooks/03-interfacing-with-python-libraries/#21-when-to-use-numpy-with-spark","title":"2.1 When to Use numpy with Spark\u00b6","text":"<p>The numpy library provides fast, vectorized array operations on your local machine. While Spark doesn\u2019t distribute pure numpy arrays across the cluster, you can still use numpy in combination with Spark for:</p> <ul> <li>Local matrix transformations after collecting or sampling from Spark.</li> <li>Helper functions (e.g., random number generation, complex math) on small pieces of data.</li> </ul> <p>If your data is truly massive, you typically rely on Spark transformations or Spark MLlib. But for smaller subsets or specialized numeric routines, numpy remains indispensable.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#22-example-converting-spark-data-to-numpy-arrays","title":"2.2 Example: Converting Spark Data to numpy Arrays\u00b6","text":"<p>Below is an example of how you might take a Spark DataFrame, filter down or limit the rows, and then convert to a numpy array for custom local processing. This is common if you want to feed the data into a library that only accepts numpy arrays (e.g., certain specialized Python libraries without Spark integration).</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#3-scikit-learn-and-spark","title":"3. scikit-learn and Spark\u00b6","text":""},{"location":"notebooks/03-interfacing-with-python-libraries/#31-overview","title":"3.1 Overview\u00b6","text":"<p>scikit-learn is a single-machine machine learning library widely used in the Python ecosystem. It offers a broad range of supervised and unsupervised algorithms (e.g., linear regression, SVM, random forest, clustering, etc.).</p> <p>Spark, on the other hand, provides Spark MLlib\u2014a distributed, scalable machine learning library that can handle datasets too large for a single machine.</p> <ul> <li>If your data fits in memory on a single machine, you might just collect it to the driver and use scikit-learn.</li> <li>If your data is too large or you need distributed training, use Spark MLlib.</li> </ul>"},{"location":"notebooks/03-interfacing-with-python-libraries/#32-using-scikit-learn-locally-after-collecting-data","title":"3.2 Using scikit-learn Locally After Collecting Data\u00b6","text":"<p>Below is a simple example demonstrating how you could train a linear regression model in scikit-learn after collecting Spark data locally. This approach is feasible only if the dataset is small enough.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#33-spark-mllib-vs-scikit-learn","title":"3.3 Spark MLlib vs. scikit-learn\u00b6","text":"<p>Spark MLlib is designed to scale across multiple nodes. It uses DataFrames and distributed computing to train models on data that might be terabytes in size. scikit-learn is widely used but has a single-machine approach.</p> <ul> <li><p>Spark MLlib advantages:</p> <ol> <li>Distributable algorithms for huge datasets.</li> <li>Seamless integration with Spark DataFrames.</li> <li>Built-in support for pipelines, parameter tuning, etc.</li> </ol> </li> <li><p>scikit-learn advantages:</p> <ol> <li>Massive library of algorithms and features (wider than MLlib).</li> <li>Simpler debugging and familiarity for many data scientists.</li> <li>Large ecosystem of user-contributed modules.</li> </ol> </li> </ul> <p>In practice, if your dataset can fit on one machine (or you can do partial sampling or chunk-based approaches), scikit-learn is a great choice. If you need to handle truly big data in parallel, Spark MLlib can help you avoid memory bottlenecks on a single node.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#4-additional-considerations","title":"4. Additional Considerations\u00b6","text":""},{"location":"notebooks/03-interfacing-with-python-libraries/#41-data-serialization","title":"4.1 Data Serialization\u00b6","text":"<p>When passing data between Spark and local Python processes, data must be serialized\u2014converted to a format for transmission or collection. This overhead can become significant if you frequently shuttle large amounts of data back and forth. To minimize overhead, try to:</p> <ul> <li>Perform as many transformations as possible in Spark.</li> <li>Only collect subsets or aggregated results locally.</li> <li>Avoid tight loops in Python that call Spark operations repeatedly\u2014batch them if possible.</li> </ul>"},{"location":"notebooks/03-interfacing-with-python-libraries/#42-udf-performance","title":"4.2 UDF Performance\u00b6","text":"<p>Some libraries (like numpy, scikit-learn) may be tempting to call in a Spark User-Defined Function (UDF). However, be aware that UDFs can bypass Spark\u2019s internal optimizations. If an equivalent function exists in <code>pyspark.sql.functions</code>, prefer the built-in Spark function.</p> <p>That said, if you truly need custom logic not available in Spark, UDFs or pandas UDFs can be a valid approach. Just be mindful of potential performance trade-offs.</p>"},{"location":"notebooks/03-interfacing-with-python-libraries/#5-summary","title":"5. Summary\u00b6","text":""},{"location":"notebooks/03-interfacing-with-python-libraries/#51-key-takeaways","title":"5.1 Key Takeaways\u00b6","text":"<ol> <li>pandas \u2194 Spark: Use <code>spark_df.toPandas()</code> and <code>spark.createDataFrame(pandas_df)</code> to move data in and out of Spark. Great for smaller datasets or local manipulations.</li> <li>numpy with Spark: Perfect for specialized numeric tasks on subsets of data. Full distribution of numpy arrays is not supported, so you often rely on Spark DataFrame transformations for big data.</li> <li>scikit-learn vs. Spark MLlib: scikit-learn is single-node and offers a vast algorithm library, but Spark MLlib distributes training across a cluster to handle larger datasets.</li> <li>Keep Data in Spark: For large-scale pipelines, keep as much data as possible in Spark to avoid serialization overhead.</li> </ol>"},{"location":"notebooks/03-interfacing-with-python-libraries/#52-next-steps","title":"5.2 Next Steps\u00b6","text":"<ul> <li>If you plan on applying machine learning at scale, explore Spark MLlib and how it compares in API and capabilities to scikit-learn.</li> <li>Consider trying out pyspark.pandas for a more pandas-like experience on large datasets.</li> <li>Explore pandas UDFs if you need to apply vectorized computations in a distributed manner.</li> </ul> <p>In the next notebook, we\u2019ll explore visualization and statsmodels integration to see how you can generate plots and run statistical tests while still leveraging Spark.</p>"}]}