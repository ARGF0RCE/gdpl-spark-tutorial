{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Interfacing with Python Libraries\n",
        "\n",
        "In this notebook, we will explore how **Apache Spark** integrates with common Python data science libraries, specifically:\n",
        "\n",
        "1. [pandas](https://pandas.pydata.org/)\n",
        "2. [numpy](https://numpy.org/)\n",
        "3. [scikit-learn](https://scikit-learn.org/)\n",
        "\n",
        "You’ll learn:\n",
        "\n",
        "- How to **convert Spark DataFrames** to and from pandas DataFrames.\n",
        "- How to **leverage** PySpark for distributing data across your cluster.\n",
        "- When (and why) you’d want to **collect** smaller datasets locally to do numpy-based manipulation.\n",
        "- **Comparisons** between Spark MLlib and scikit-learn.\n",
        "- Techniques for **scaling** your machine learning tasks beyond a single node.\n",
        "\n",
        "By the end, you’ll have a robust understanding of how Spark can coexist in the typical Python data science ecosystem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Prerequisites\n",
        "\n",
        "This notebook assumes:\n",
        "\n",
        "- You have a **SparkSession** available as `spark`.\n",
        "- You have installed relevant Python libraries (`pandas`, `numpy`, `scikit-learn`).\n",
        "- You understand basic Spark concepts like **DataFrames**, **RDDs**, **transformations**, and **actions**.\n",
        "\n",
        "If you haven’t already, please refer to previous notebooks for setup and Spark basics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. pandas and Spark\n",
        "\n",
        "### 1.1 Converting Spark DataFrames to pandas DataFrames\n",
        "\n",
        "Often, you’ll want to perform local operations on a smaller subset of data that easily fits in memory on a single machine. In such scenarios, you can **collect** that data as a **pandas** DataFrame. The `toPandas()` method is the simplest approach for doing so.\n",
        "\n",
        "> **Warning**: `toPandas()` will bring all data into the driver’s memory, so ensure the dataset isn’t too large.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession.builder.appName(\"InterfacingWithPythonLibs\").getOrCreate()\n",
        "\n",
        "# Example Spark DataFrame\n",
        "data = [\n",
        "    (\"Alice\", 29, \"Engineer\"),\n",
        "    (\"Bob\",   35, \"Doctor\"),\n",
        "    (\"Cathy\", 25, \"Artist\"),\n",
        "    (\"David\", 42, \"Chef\")\n",
        "]\n",
        "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
        "\n",
        "spark_df = spark.createDataFrame(data, columns)\n",
        "print(\"Spark DataFrame:\")\n",
        "spark_df.show()\n",
        "\n",
        "# Convert to a pandas DataFrame (Collects all data locally!)\n",
        "pdf = spark_df.toPandas()\n",
        "print(\"\\nConverted to pandas DataFrame:\")\n",
        "print(pdf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above code snippet:\n",
        "\n",
        "- Creates a simple Spark DataFrame with 4 rows.\n",
        "- Uses `.toPandas()` to bring it to the driver machine.\n",
        "\n",
        "In real-world scenarios, you might **filter** or **sample** your Spark DataFrame first, so that only a manageable subset is converted to pandas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Converting pandas DataFrames to Spark DataFrames\n",
        "\n",
        "If you have local data in pandas (for example, from CSV files that are small enough to fit on a single machine), you can **create** a Spark DataFrame using the `spark.createDataFrame()` method.\n",
        "\n",
        "> **Tip**: Some overhead exists when converting from pandas to Spark—Spark has to infer or confirm the schema. For large data, consider loading directly into Spark from the source instead of going through pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Let's assume you already have a local pandas DataFrame 'pdf'\n",
        "pdf_extra = pd.DataFrame({\n",
        "    'Name': [\"Evelyn\", \"Frank\"],\n",
        "    'Age': [31, 28],\n",
        "    'Occupation': [\"Manager\", \"Nurse\"]\n",
        "})\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "spark_df_extra = spark.createDataFrame(pdf_extra)\n",
        "\n",
        "print(\"New Spark DataFrame created from pandas:\")\n",
        "spark_df_extra.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 pandas API on Spark (pyspark.pandas)\n",
        "\n",
        "Starting in Spark 3.2+, you can use **pandas APIs on Spark** (often referred to as **`pyspark.pandas`**). This is an **experimental** feature that attempts to let you write **pandas-like code** but execute it **distributively** on Spark. That way, you can handle bigger-than-memory data with familiar pandas syntax.\n",
        "\n",
        "> **Note**: Performance might differ from pure Spark SQL, but for many users who love pandas, this can ease the learning curve.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pyspark.pandas as ps\n",
        "\n",
        "# Create a pandas-on-Spark DataFrame\n",
        "psdf = ps.DataFrame({\n",
        "    'colA': [10, 20, 30],\n",
        "    'colB': [100, 200, 300]\n",
        "})\n",
        "print(\"pandas-on-Spark DataFrame:\")\n",
        "print(psdf)\n",
        "\n",
        "# You can do typical pandas operations, e.g.,\n",
        "print(\"\\npsdf.sum():\")\n",
        "print(psdf.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Behind the scenes, `pyspark.pandas` transforms your calls into Spark transformations. This allows scaling out to large datasets that exceed normal pandas memory constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. numpy and Spark\n",
        "\n",
        "### 2.1 When to Use numpy with Spark\n",
        "\n",
        "The **numpy** library provides fast, vectorized array operations on your local machine. While Spark doesn’t distribute pure numpy arrays across the cluster, you can still use numpy in combination with Spark for:\n",
        "\n",
        "- **Local matrix transformations** after collecting or sampling from Spark.\n",
        "- **Helper functions** (e.g., random number generation, complex math) on small pieces of data.\n",
        "\n",
        "If your data is truly massive, you typically rely on Spark transformations or Spark MLlib. But for smaller subsets or specialized numeric routines, numpy remains indispensable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Example: Converting Spark Data to numpy Arrays\n",
        "\n",
        "Below is an example of how you might take a Spark DataFrame, filter down or limit the rows, and then convert to a numpy array for custom local processing. This is common if you want to feed the data into a library that **only** accepts numpy arrays (e.g., certain specialized Python libraries without Spark integration).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Suppose 'spark_df' is large. We'll create a smaller sample.\n",
        "small_spark_df = spark_df.limit(2)  # just 2 rows for demonstration\n",
        "\n",
        "# Collect to driver as a list of Row objects\n",
        "rows = small_spark_df.collect()\n",
        "\n",
        "# Convert rows to a structured numpy array, or just a list for further processing\n",
        "names = [row['Name'] for row in rows]\n",
        "ages = np.array([row['Age'] for row in rows])\n",
        "\n",
        "print(\"Names (list):\", names)\n",
        "print(\"Ages (numpy array):\", ages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your dataset is large, you might instead prefer distributed transformations in Spark MLlib. But for smaller tasks or specialized numeric calculations, this approach is straightforward.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. scikit-learn and Spark\n",
        "\n",
        "### 3.1 Overview\n",
        "\n",
        "[scikit-learn](https://scikit-learn.org/) is a **single-machine** machine learning library widely used in the Python ecosystem. It offers a broad range of supervised and unsupervised algorithms (e.g., linear regression, SVM, random forest, clustering, etc.).\n",
        "\n",
        "Spark, on the other hand, provides **Spark MLlib**—a distributed, scalable machine learning library that can handle datasets too large for a single machine.\n",
        "\n",
        "- If your data **fits in memory** on a single machine, you might just collect it to the driver and use **scikit-learn**.\n",
        "- If your data is **too large** or you need distributed training, use **Spark MLlib**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Using scikit-learn Locally After Collecting Data\n",
        "\n",
        "Below is a simple example demonstrating how you could train a **linear regression** model in scikit-learn after collecting Spark data locally. This approach is feasible only if the dataset is small enough.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose we have a Spark DF with numeric columns: e.g., 'Feature1', 'Feature2', 'Label'\n",
        "sample_data_for_sklearn = [\n",
        "    (1.0, 10.0,  8.5),\n",
        "    (2.0, 18.0, 14.0),\n",
        "    (3.0, 23.0, 17.2),\n",
        "    (4.0, 28.0, 20.0),\n",
        "    (5.0, 39.0, 32.0)\n",
        "]\n",
        "schema_cols = [\"Feature1\", \"Feature2\", \"Label\"]\n",
        "df_sklearn = spark.createDataFrame(sample_data_for_sklearn, schema_cols)\n",
        "\n",
        "# Collect to driver in pandas\n",
        "pdf_sklearn = df_sklearn.toPandas()\n",
        "\n",
        "# Separate features and labels\n",
        "X = pdf_sklearn[[\"Feature1\", \"Feature2\"]]\n",
        "y = pdf_sklearn[\"Label\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit scikit-learn model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "score = lr_model.score(X_test, y_test)\n",
        "print(\"R-squared on test set:\", score)\n",
        "print(\"Coefficients:\", lr_model.coef_)\n",
        "print(\"Intercept:\", lr_model.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you run the above:\n",
        "\n",
        "- Spark will collect all rows (5 in this example) into a pandas DataFrame.\n",
        "- We use `train_test_split` and `LinearRegression` from scikit-learn to fit a simple model.\n",
        "\n",
        "Again, this is only advisable for small to moderate datasets that your driver machine can handle in memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Spark MLlib vs. scikit-learn\n",
        "\n",
        "Spark MLlib is designed to **scale** across multiple nodes. It uses DataFrames and distributed computing to train models on data that might be terabytes in size. scikit-learn is widely used but has a **single-machine** approach.\n",
        "\n",
        "- **Spark MLlib** advantages:\n",
        "  1. Distributable algorithms for huge datasets.\n",
        "  2. Seamless integration with Spark DataFrames.\n",
        "  3. Built-in support for pipelines, parameter tuning, etc.\n",
        "\n",
        "- **scikit-learn** advantages:\n",
        "  1. Massive library of algorithms and features (wider than MLlib).\n",
        "  2. Simpler debugging and familiarity for many data scientists.\n",
        "  3. Large ecosystem of user-contributed modules.\n",
        "\n",
        "In practice, if your dataset can fit on one machine (or you can do partial sampling or chunk-based approaches), scikit-learn is a great choice. If you need to handle truly **big data** in parallel, Spark MLlib can help you avoid memory bottlenecks on a single node.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Additional Considerations\n",
        "\n",
        "### 4.1 Data Serialization\n",
        "\n",
        "When passing data between Spark and local Python processes, data must be **serialized**—converted to a format for transmission or collection. This overhead can become significant if you frequently shuttle large amounts of data back and forth. To minimize overhead, try to:\n",
        "\n",
        "- Perform **as many transformations** as possible in Spark.\n",
        "- Only **collect** subsets or aggregated results locally.\n",
        "- Avoid tight loops in Python that call Spark operations repeatedly—batch them if possible.\n",
        "\n",
        "### 4.2 UDF Performance\n",
        "\n",
        "Some libraries (like numpy, scikit-learn) may be tempting to call in a Spark **User-Defined Function (UDF)**. However, be aware that UDFs can bypass Spark’s internal optimizations. If an equivalent function exists in `pyspark.sql.functions`, prefer the built-in Spark function.\n",
        "\n",
        "That said, if you truly need custom logic not available in Spark, UDFs or **pandas UDFs** can be a valid approach. Just be mindful of potential performance trade-offs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary\n",
        "\n",
        "### 5.1 Key Takeaways\n",
        "1. **pandas ↔ Spark**: Use `spark_df.toPandas()` and `spark.createDataFrame(pandas_df)` to move data in and out of Spark. Great for smaller datasets or local manipulations.\n",
        "2. **numpy with Spark**: Perfect for specialized numeric tasks on subsets of data. Full distribution of numpy arrays is not supported, so you often rely on Spark DataFrame transformations for big data.\n",
        "3. **scikit-learn vs. Spark MLlib**: scikit-learn is single-node and offers a vast algorithm library, but Spark MLlib distributes training across a cluster to handle larger datasets.\n",
        "4. **Keep Data in Spark**: For large-scale pipelines, keep as much data as possible in Spark to avoid serialization overhead.\n",
        "\n",
        "### 5.2 Next Steps\n",
        "\n",
        "- If you plan on applying **machine learning** at scale, explore **Spark MLlib** and how it compares in **API** and **capabilities** to scikit-learn.\n",
        "- Consider trying out **pyspark.pandas** for a more pandas-like experience on large datasets.\n",
        "- Explore **pandas UDFs** if you need to apply vectorized computations in a distributed manner.\n",
        "\n",
        "In the next notebook, we’ll explore **visualization** and **statsmodels** integration to see how you can generate plots and run statistical tests while still leveraging Spark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# (Optional) Stop the Spark session when you're done.\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}