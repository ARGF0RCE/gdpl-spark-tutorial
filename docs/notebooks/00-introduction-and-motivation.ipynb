{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Introduction and Motivation\n",
        "\n",
        "Apache Spark has become one of the most popular frameworks for **large-scale data processing** and advanced analytics. It is widely adopted by companies such as **Netflix**, **Uber**, **Airbnb**, and more, primarily because:\n",
        "\n",
        "- It offers **in-memory** computation, which can be significantly faster than traditional disk-based systems.\n",
        "- It has a **unified engine** for both batch and streaming data.\n",
        "- It provides **high-level APIs** in Python, Scala, Java, and R for accessible big data analytics.\n",
        "\n",
        "## Real-World Spark Use Cases\n",
        "\n",
        "1. **Netflix**:\n",
        "   - Spark is used to build and run Netflix’s recommendation engine, analyzing user watching patterns and behaviors in near real-time.\n",
        "   - Streams of event data (when you pause, play, skip, or interact with a video) are ingested, then processed with Spark to generate content suggestions.\n",
        "   - Spark’s in-memory engine drastically reduces the time to train and refresh large-scale machine learning models.\n",
        "\n",
        "2. **Uber**:\n",
        "   - Uber leverages Spark (alongside Kafka and other data pipelines) to handle real-time analytics for ride pricing, supply-demand forecasting, and route optimization.\n",
        "   - Spark’s streaming capabilities help them react quickly to changing traffic conditions, user surge, and more.\n",
        "\n",
        "3. **Airbnb**:\n",
        "   - Airbnb uses Spark to analyze user bookings, prices, and host preferences.\n",
        "   - They apply machine learning models on huge datasets for personalized search rankings and dynamic pricing.\n",
        "\n",
        "4. **E-commerce Platforms**:\n",
        "   - Large online retailers use Spark for recommendation engines, A/B testing analysis, and fraud detection.\n",
        "   - Data streams (clicks, orders, reviews) can be combined and processed in near real-time for better user experiences.\n",
        "\n",
        "Spark’s ability to distribute processing across a cluster allows you to tackle massive datasets that would be infeasible to handle on a single machine. Additionally, Spark’s ecosystem includes libraries like **Spark SQL**, **Spark Streaming**, **MLlib**, and **GraphX**, which turn Spark into a one-stop solution for broad data processing tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## Comparing Apache Spark with Hadoop\n",
        "\n",
        "While both Apache Spark and Hadoop are powerful tools for big data processing, they have distinct differences in architecture, performance, and use cases.\n",
        "\n",
        "### Diagram: Apache Spark vs. Hadoop Architecture\n",
        "\n",
        "![Apache Spark vs. Hadoop Architecture](../images/spark_vs_hadoop_arch.jpg)\n",
        "\n",
        "![Apache Spark vs. Hadoop Ecosystem](../images/spark_vs_hadoop_arch_2.png)\n",
        "\n\n",
        "*Reference for more information: [GeeksforGeeks](https://www.geeksforgeeks.org/difference-between-hadoop-and-spark/)*\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "- **Processing Model**:\n",
        "  - **Hadoop**: Utilizes a disk-based storage system (HDFS) and processes data in batches using the MapReduce paradigm. Each operation reads from and writes to disk, which can introduce latency.\n",
        "  - **Spark**: Employs in-memory processing, keeping data in RAM between operations. This approach significantly reduces read/write cycles to disk, enhancing performance, especially for iterative tasks.\n",
        "\n",
        "- **Performance**:\n",
        "  - **Hadoop**: Suitable for large-scale batch processing but may experience slower performance due to disk I/O operations.\n",
        "  - **Spark**: Can be up to 100 times faster than Hadoop for certain tasks, thanks to in-memory computation. This speed advantage is particularly noticeable in machine learning algorithms and real-time data processing.\n",
        "\n",
        "- **Fault Tolerance**:\n",
        "  - **Hadoop**: Achieves fault tolerance through data replication across multiple nodes. If a node fails, data can be retrieved from another replica.\n",
        "  - **Spark**: Uses Resilient Distributed Datasets (RDDs) with lineage information, allowing it to recompute lost data without the need for replication, thus saving storage space.\n",
        "\n",
        "- **Ease of Use**:\n",
        "  - **Hadoop**: Requires complex code, often in Java, making it less accessible for rapid development.\n",
        "  - **Spark**: Provides high-level APIs in multiple languages, including Python, Scala, and R, simplifying the development process.\n",
        "\n",
        "### Diagram: Performance Comparison\n",
        "\n",
        "![Performance Comparison](../images/performance_comparison.png)\n",
        "\n\n",
        "*Reference for more information: [Data Engineer Academy](https://dataengineeracademy.com/blog/apache-spark-vs-hadoop-comprehensive-guide/)*\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "- **Batch Processing**:\n",
        "  - **Hadoop**: Efficient for processing large volumes of data in batches but may not be ideal for real-time analytics.\n",
        "  - **Spark**: Excels in both batch and real-time processing, offering flexibility for various data processing needs.\n",
        "\n",
        "- **Scalability**:\n",
        "  - Both frameworks are highly scalable, capable of handling petabytes of data across numerous nodes. However, Spark's in-memory requirements can lead to higher costs due to the need for more RAM.\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "Hadoop use cases include:\n",
        "\n",
        "- Processing large datasets in environments where data size exceeds available memory.\n",
        "- Building data analysis infrastructure with a limited budget.\n",
        "- Completing jobs where immediate results are not required, and time is not a limiting factor.\n",
        "- Batch processing with tasks exploiting disk read and write operations.\n",
        "- Historical and archive data analysis.\n",
        "- With Spark, we can separate the following use cases where it outperforms Hadoop:\n",
        "\n",
        "- The analysis of real-time stream data.\n",
        "- When time is of the essence, Spark delivers quick results with in-memory computations.\n",
        "- Dealing with the chains of parallel operations using iterative algorithms.\n",
        "- Graph-parallel processing to model the data.\n",
        "- All machine learning applications.\n",
        "\n",
        "*Reference for more information: [PhoenixNAP](https://phoenixnap.com/kb/hadoop-vs-spark)*\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "- **Machine Learning**:\n",
        "  - **Hadoop**: Limited machine learning capabilities through libraries like Mahout. Not ideal for iterative algorithms.\n",
        "  - **Spark**: Offers MLlib for scalable machine learning tasks, supporting iterative algorithms and real-time processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-World Spark Use Cases\n",
        "1. **Netflix**: Recommendation systems, real-time user behavior analysis.\n",
        "2. **Uber**: Real-time analytics for pricing, supply-demand forecasting.\n",
        "3. **Airbnb**: User personalization, dynamic pricing.\n",
        "4. **E-commerce Platforms**: Fraud detection, recommendation engines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Python Example\n",
        "\n",
        "Below is a *trivial* code snippet that processes a random dataset with pure Python loops or standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "N = 10_000_000\n",
        "data = [random.random() for _ in range(N)]\n",
        "\n",
        "start = time.time()\n",
        "mean_val = sum(data) / len(data)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Mean: {mean_val}\")\n",
        "print(f\"Time taken (pure Python): {end - start:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Spark Example\n",
        "\n",
        "Now let's see how Spark can help us handle even larger data seamlessly, leveraging distributed computing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkIntro\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Convert the Python list to an RDD\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "start_spark = time.time()\n",
        "mean_val_spark = rdd.mean()\n",
        "end_spark = time.time()\n",
        "\n",
        "print(f\"Spark Mean: {mean_val_spark}\")\n",
        "print(f\"Time taken (Spark): {end_spark - start_spark:.2f} seconds\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}