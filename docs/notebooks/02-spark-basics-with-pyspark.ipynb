{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Spark Basics with PySpark\n",
        "\n",
        "In this notebook, we’ll learn the **building blocks** of Spark using **PySpark**:\n",
        "\n",
        "1. **SparkSession**: The entry point to Spark.\n",
        "2. **RDD (Resilient Distributed Datasets)**: The lower-level abstraction.\n",
        "3. **DataFrames**: The higher-level, SQL-like abstraction.\n",
        "4. **Transformations and Actions**: The functional operations that drive Spark computations.\n",
        "5. **Partitioning and Persistence**: How Spark manages data distribution.\n",
        "\n",
        "By the end, you’ll understand the basics of how PySpark organizes and processes data at scale.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Creating a SparkSession\n",
        "\n",
        "The **SparkSession** is Spark’s main entry point in **Spark 2.0+**. It allows you to create DataFrames, register DataFrame tables, execute SQL queries, and read from external data sources.\n",
        "\n",
        "**Additional Explanation**\n",
        "\n",
        "Before Spark 2.0, you would initialize Spark with separate entries (e.g., `SparkContext`, `SQLContext`, `HiveContext`). The `SparkSession` conveniently unified these contexts into a single entry point. This consolidation means you can seamlessly switch between DataFrame operations, SQL queries, and lower-level RDD manipulations without re-initializing or juggling multiple contexts.\n",
        "\n",
        "When you build your SparkSession, you can also configure:\n",
        "- **Master URL** (e.g., `local[*]`, `spark://...`) to specify the cluster manager.\n",
        "- **Config options** like memory usage, shuffle partitions, or log level.\n",
        "\n",
        "For example, to set the log level lower (for less verbose output) and specify 4 local cores:\n",
        "```python\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkBasics\") \\\n",
        "    .master(\"local[4]\") \\\n",
        "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "```\n",
        "\n",
        "Below, we create a SparkSession named `SparkBasics`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create or get a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkBasics\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"SparkSession created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note**: If you’re running this locally, ensure `SPARK_HOME` is set correctly and `pyspark` is installed. On a cloud environment or in a notebook like Databricks, a SparkSession may already be created for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding RDDs (Resilient Distributed Datasets)\n",
        "\n",
        "RDDs are the **lower-level** abstraction in Spark, representing a distributed collection of items. They provide **fault tolerance** (resilience) and can be operated on in parallel across a cluster. Nowadays, you’ll mostly use **DataFrames**, but RDDs are still useful for specialized or custom operations.\n",
        "\n",
        "### 2.1 Creating RDDs\n",
        "\n",
        "You can create an RDD in several ways:\n",
        "1. **Parallelize** a local collection (e.g., a Python list).\n",
        "2. **Load** external data (text files, CSV, etc.) via `sparkContext.textFile(...)`.\n",
        "\n",
        "Below, we create an RDD from a local list:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an RDD by parallelizing a local list\n",
        "data_list = [\"apple\", \"banana\", \"cherry\", \"date\"]\n",
        "rdd = spark.sparkContext.parallelize(data_list)\n",
        "print(\"RDD count:\", rdd.count())\n",
        "print(\"RDD sample:\", rdd.take(2))  # take(2) fetches first 2 elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "RDDs are the fundamental data structure at the heart of Spark’s distributed engine. Each RDD is conceptually an immutable collection of data partitioned across nodes in the cluster. Spark automatically tracks the **lineage** of each RDD—that is, the sequence of operations used to create it—making it easy to recompute partitions if some node fails.\n",
        "\n",
        "You can also create RDDs from:\n",
        "- **Text files**:\n",
        "  ```python\n",
        "  text_rdd = spark.sparkContext.textFile(\"path/to/myfile.txt\")\n",
        "  ```\n",
        "- **Whole directories** of data:\n",
        "  ```python\n",
        "  large_rdd = spark.sparkContext.textFile(\"hdfs://path/to/huge-dataset/*\")\n",
        "  ```\n",
        "\n",
        "When working locally, you can load files from your machine’s filesystem. For cluster deployments, you typically load from HDFS, S3, or another distributed store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Transformations and Actions on RDDs\n",
        "\n",
        "Spark’s **transformations** return a new RDD (they’re **lazy**), while **actions** trigger execution and return a value (materializing the result). Common transformations include `map()`, `filter()`, and `flatMap()`. Common actions are `collect()`, `count()`, `reduce()`, etc.\n",
        "\n",
        "Below, we apply transformations to an RDD and then use an action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RDD Transformations\n",
        "mapped_rdd = rdd.map(lambda x: x.upper())   # map() transforms each element\n",
        "filtered_rdd = mapped_rdd.filter(lambda x: x.startswith(\"B\"))  # filter() keeps certain elements\n",
        "\n",
        "# RDD Action\n",
        "result = filtered_rdd.collect()  # collect() returns all elements to the driver\n",
        "\n",
        "print(\"Transformed RDD Result:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "- **Transformations**: Create a *new* RDD by defining how each record is mapped from the parent RDD. Examples include:\n",
        "  - `map()`: Apply a function to each element.\n",
        "  - `filter()`: Keep only elements passing a predicate.\n",
        "  - `flatMap()`: Similar to `map()` but allows splitting elements into multiple outputs.\n",
        "- **Actions**: Trigger execution and return a value. Examples include:\n",
        "  - `reduce(func)`: Combine elements using a user-specified function that operates on two items at a time.\n",
        "  - `count()`: Return the number of elements in the RDD.\n",
        "  - `first()`, `take(n)`: Retrieve elements to the driver program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. DataFrames: The Higher-Level Abstraction\n",
        "\n",
        "DataFrames build on top of RDDs and provide a **relational** view of data, with named columns and powerful optimizations via the **Catalyst** query optimizer. They’re generally **faster** and **easier to use** for most big data tasks.\n",
        "\n",
        "### 3.1 Creating a DataFrame\n",
        "You can create a DataFrame from:\n",
        "- **Python lists** (small data) or RDDs.\n",
        "- **External data sources** (CSV, JSON, Parquet, etc.).\n",
        "\n",
        "Here’s a simple example from a Python list of tuples:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_data = [\n",
        "    (\"Alice\", 29, \"Engineer\"),\n",
        "    (\"Bob\",   35, \"Doctor\"),\n",
        "    (\"Cathy\", 25, \"Artist\")\n",
        "]\n",
        "\n",
        "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
        "df = spark.createDataFrame(sample_data, columns)\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "While you can create DataFrames from local data, real-world use often comes from:\n",
        "1. **Reading CSV/JSON/Parquet**:\n",
        "   ```python\n",
        "   df_csv = spark.read.csv(\"path/to/data.csv\", header=True, inferSchema=True)\n",
        "   df_parquet = spark.read.parquet(\"path/to/data.parquet\")\n",
        "   ```\n",
        "2. **SQL Tables** (via JDBC/ODBC or Hive Metastore):\n",
        "   ```python\n",
        "   jdbc_df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://hostname/db\") \\\n",
        "    .option(\"dbtable\", \"tablename\") \\\n",
        "    .option(\"user\", \"username\") \\\n",
        "    .option(\"password\", \"secret\") \\\n",
        "    .load()\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DataFrames also enable quick ETL (Extract, Transform, Load) patterns if you need to combine multiple datasets. By specifying the schema or letting Spark infer it, you keep metadata about column names, types, etc.\n",
        "\n",
        "Under the hood, Spark still uses RDDs for low-level operations. But for day-to-day usage, DataFrames are often more concise and more performant thanks to the Catalyst Optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Inspecting DataFrames\n",
        "Use methods like `.show()`, `.describe()`, and `.printSchema()` to explore DataFrames:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print schema\n",
        "df.printSchema()\n",
        "\n",
        "# Summaries\n",
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "You can also use:\n",
        "- `df.columns` to get a list of column names.\n",
        "- `df.dtypes` to see columns and their Spark data types.\n",
        "- `df.head(n)` or `df.take(n)` to return the first `n` rows locally (similar to `.show(n)` but returns a list of Row objects).\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "print(\"Columns:\", df.columns)\n",
        "print(\"Schema (dtypes):\", df.dtypes)\n",
        "for row in df.head(2):\n",
        "    print(row)\n",
        "```\n",
        "\n",
        "These methods make it easy to quickly inspect your DataFrame structure and sample data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 DataFrame Operations\n",
        "DataFrames support a wide variety of operations, including **selecting columns**, **filtering**, **grouping**, and **aggregation**. Many of these are similar to SQL queries.\n",
        "\n",
        "Below is an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select columns\n",
        "df.select(\"Name\", \"Age\").show()\n",
        "\n",
        "# Filter rows\n",
        "df.filter(df.Age > 30).show()\n",
        "\n",
        "# Group + Agg\n",
        "df.groupBy(\"Occupation\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "- **Column Expressions**: You can reference columns via `df.colName` or using Spark’s `F` (functions) library:\n",
        "  ```python\n",
        "  from pyspark.sql.functions import col, lower, upper\n",
        "  new_df = df.select(upper(col(\"Name\")).alias(\"NAME_UPPER\"))\n",
        "  new_df.show()\n",
        "  ```\n",
        "- **User-Defined Functions (UDFs)**: For more complex transformations, define UDFs:\n",
        "    ```python\n",
        "    from pyspark.sql.functions import udf\n",
        "    from pyspark.sql.types import StringType\n",
        "\n",
        "    def greet(name):\n",
        "        return f\"Hello, {name}!\"\n",
        "\n",
        "    greet_udf = udf(greet, StringType())\n",
        "    df.select(\"Name\", greet_udf(col(\"Name\")).alias(\"Greeting\")).show()\n",
        "    ```\n",
        "> **Note**: However, keep in mind that UDFs can be slower than built-in Spark functions, since they bypass many of Spark’s optimizations.\n",
        "\n",
        "- **Joins**: DataFrames support `inner`, `left`, `right`, `full`, and `cross` joins. Example:\n",
        "    ```python\n",
        "    df1.join(df2, df1.id == df2.id, \"inner\").show()\n",
        "    ```\n",
        "Use the join mode that suits your data relationship to handle missing or unmatched records appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformations and Actions on DataFrames\n",
        "\n",
        "While RDDs have functional transformations, DataFrames offer a more **SQL-like** syntax for transformations. Actions (like `.show()`) will execute the query plan.\n",
        "\n",
        "> **Example**: Filtering rows, creating new columns, or performing aggregations are transformations. Calling `.collect()` or `.count()` is an action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "- **Transformations**: Spark builds a logical plan describing how to transform data. For instance:\n",
        "  - `select()`, `filter()`, `withColumn()`, `groupBy()`, `agg()`\n",
        "- **Actions**: Evaluate the plan. Examples:\n",
        "  - `collect()`, `count()`, `show()`, `head()`\n",
        "  \n",
        "**Catalyst Query Optimizer**  \n",
        "Spark’s DataFrame operations are compiled down to an optimized plan thanks to the Catalyst optimizer. It can rearrange filters, push down predicates, and combine operations for efficiency. This is why using DataFrame APIs is typically faster than manual RDD manipulations for most SQL-like workflows.\n",
        "\n",
        "**Example**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_filtered = df.filter(col(\"Age\") > 30)\n",
        "row_count = df_filtered.count()  # triggers execution\n",
        "print(f\"Number of records with Age > 30: {row_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Partitioning and Persistence\n",
        "\n",
        "Spark automatically partitions DataFrames/RDDs across the cluster. On a **single machine** or small environment, you might not notice it as much, but in a **distributed** setting, partitioning is crucial for parallelism.\n",
        "\n",
        "You can **cache** or **persist** frequently accessed data to improve performance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of caching a DataFrame\n",
        "df.cache()  # or df.persist()\n",
        "# Now subsequent actions on df will be faster if the data is reused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "- **Partitioning**: Spark splits data into partitions which are processed in parallel across the cluster (or local CPU cores). If you have a large dataset, you can adjust the number of partitions:\n",
        "  ```python\n",
        "  df = df.repartition(8)  # Increase partition count\n",
        "  ```\n",
        "- Conversely, you can reduce partitions using coalesce() if you want fewer, bigger partitions. Partitioning affects shuffle performance and parallelism.\n",
        "\n",
        "Persistence Levels: By default, `cache()` uses MEMORY_ONLY storage. You can choose other persistence levels:\n",
        "- `MEMORY_AND_DISK`\n",
        "- `MEMORY_ONLY_SER`\n",
        "- `DISK_ONLY`\n",
        "\n",
        "> Depending on data size and memory constraints, picking the right level is essential for performance. For instance, if your data is larger than available memory, `MEMORY_AND_DISK` can help avoid `OutOfMemoryError`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark import StorageLevel\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)  # or MEMORY_ONLY, MEMORY_ONLY_SER, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Shutting Down Spark\n",
        "\n",
        "When you’re finished, it’s good practice to **stop** the SparkSession (especially in scripts or local dev environments):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Additional Explanation**\n",
        "\n",
        "When you call `spark.stop()`, Spark attempts to gracefully terminate all active jobs and release resources like executors and shuffle files. In production or cluster environments, you might schedule your Spark job to run, produce output, then stop once the job completes.\n",
        "\n",
        "If you’re in a multi-notebook environment (like Databricks), be mindful that calling `stop()` can affect other notebooks sharing the same SparkSession. Some cluster managers automatically handle session lifecycles, so consult the platform’s documentation if you’re unsure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "- **RDDs**: Low-level data abstraction, good for custom or specialized tasks.\n",
        "- **DataFrames**: Higher-level relational abstraction, recommended for most analytics.\n",
        "- **Transformations**: Lazy operations that define a computation plan.\n",
        "- **Actions**: Trigger execution and return results.\n",
        "- **Partitioning and Persistence**: Key to scalability and performance.\n",
        "\n",
        "Now that you know the **basics of Spark with PySpark**, you can start using Spark to handle large datasets, run queries, and transform data quickly. In the next notebook, we’ll explore how to **interface** Spark with **pandas**, **numpy**, **scikit-learn**, and more!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
