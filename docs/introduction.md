# Introduction & Syllabus

## Why Spark?

- **Scalability**: Spark can handle huge datasets by distributing work across a cluster.
- **Speed**: In-memory computations can be 10-100x faster compared to disk-based solutions like Hadoop MapReduce.
- **Flexibility**: PySpark, R, Scala, Java... Spark supports multiple languages.

## Syllabus at a Glance

0. **Introduction and Motivation**  
      - Show how Spark beats a naive Python script.
1. **Setting Up Apache Spark**  
      - Install Spark locally, in Colab, or on the cloud.
2. **Spark Basics with PySpark**  
      - RDDs, DataFrames, transformations, and actions.
3. **Interfacing with Python Libraries**  
      - Pandas, Numpy, Scikit-Learn, etc.
   
Proceed to the notebooks for hands-on **code demos**.
